# ML 



## Generative/discriminative model

> [https://en.wikipedia.org/wiki/Generative_model](https://en.wikipedia.org/wiki/Generative_model)


## Naive Bayes

> [https://www.quora.com/In-what-real-world-applications-is-Naive-Bayes-classifier-used](https://www.quora.com/In-what-real-world-applications-is-Naive-Bayes-classifier-used)


## Recommendation System

> [https://help.netflix.com/en/node/100639](https://help.netflix.com/en/node/100639)


## Kmeans ++

> [https://en.wikipedia.org/wiki/K-means%2B%2B](https://en.wikipedia.org/wiki/K-means%2B%2B)


## Loss functions

> [https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)

> [https://gombru.github.io/2018/05/23/cross_entropy_loss/](https://gombru.github.io/2018/05/23/cross_entropy_loss/)


Regression: Mean squared error, mean squared log error

Binary classification: binary cross entropy, hinge loss, squared hinge loss, focal loss

Multiclass classification: multi-class cross entropy, Sparse Multiclass Cross-Entropy Loss, Kullback Leibler Divergence Loss


## Activation function

> [https://en.wikipedia.org/wiki/Activation_function](https://en.wikipedia.org/wiki/Activation_function)


Sigmod, softmax, 

## Random forest, xgboost, adaboost

> [https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725](https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725)


## Feature selection

> [https://machinelearningmastery.com/an-introduction-to-feature-selection/](https://machinelearningmastery.com/an-introduction-to-feature-selection/)

## A/B Testing, type I, II errors

> [https://www.abtasty.com/blog/type-1-and-type-2-errors/](https://www.abtasty.com/blog/type-1-and-type-2-errors/)
